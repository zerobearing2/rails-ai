# frozen_string_literal: true

$LOAD_PATH.unshift File.expand_path("../lib", __dir__)

require "minitest/autorun"
require "minitest/pride"
require "yaml"
require "json"

# Base paths
ROOT_PATH = File.expand_path("..", __dir__)
SKILLS_PATH = File.join(ROOT_PATH, "skills")
AGENTS_PATH = File.join(ROOT_PATH, "agents")

# Helper methods for skill testing
module SkillTestHelpers
  def load_skill(domain, skill_name)
    skill_path = File.join(SKILLS_PATH, domain, "#{skill_name}.md")
    File.read(skill_path)
  end

  def parse_skill_yaml(skill_content)
    # Extract YAML front matter
    if skill_content =~ /\A---\n(.*?)\n---\n/m
      YAML.safe_load(::Regexp.last_match(1), permitted_classes: [Symbol])
    else
      {}
    end
  end

  def extract_patterns(skill_content, pattern_name)
    # Extract specific pattern from skill
    pattern_regex = %r{<pattern name="#{pattern_name}">.*?</pattern>}m
    matches = skill_content.scan(pattern_regex)
    matches.first
  end

  def extract_code_examples(skill_content)
    # Extract all code blocks
    skill_content.scan(/```(?:ruby|erb|javascript|bash)\n(.*?)```/m).map(&:first)
  end

  def skill_file_exists?(domain, skill_name)
    File.exist?(File.join(SKILLS_PATH, domain, "#{skill_name}.md"))
  end
end

# LLM Judge helpers (for integration tests)
module LLMJudgeHelpers
  # Mock LLM client for testing
  # In production, this would call actual LLM APIs
  class LLMClient
    def initialize(provider: :mock, model: nil)
      @provider = provider
      @model = model
    end

    def evaluate(prompt)
      # Mock implementation - replace with actual API calls
      case @provider
      when :mock
        mock_evaluation
      when :openai
        evaluate_with_openai(prompt)
      when :anthropic
        evaluate_with_anthropic(prompt)
      else
        raise "Unknown provider: #{@provider}"
      end
    end

    private

    def mock_evaluation
      {
        "overall_score" => 4.5,
        "pass" => true,
        "scores" => {
          "correct_pattern" => 5,
          "rails_conventions" => 4,
          "avoids_antipatterns" => 5,
          "code_quality" => 4
        },
        "issues" => [],
        "suggestions" => []
      }
    end

    def evaluate_with_openai(prompt)
      # TODO: Implement OpenAI API call
      # require 'openai'
      # client = OpenAI::Client.new(access_token: ENV['OPENAI_API_KEY'])
      # response = client.chat(parameters: { ... })
      raise NotImplementedError, "OpenAI integration not yet implemented"
    end

    def evaluate_with_anthropic(prompt)
      # TODO: Implement Anthropic API call
      # require 'anthropic'
      # client = Anthropic::Client.new(api_key: ENV['ANTHROPIC_API_KEY'])
      # response = client.messages.create(...)
      raise NotImplementedError, "Anthropic integration not yet implemented"
    end
  end

  def create_judge_prompt(skill_name, scenario, generated_code)
    <<~PROMPT
      You are evaluating code generated by an AI agent that was given the #{skill_name} skill.

      Evaluate the following aspects on a scale of 1-5:
      1. Correct Pattern Usage - Does it use the expected patterns from the skill?
      2. Rails 8.1+ Conventions - Does it follow modern Rails best practices?
      3. Avoids Antipatterns - Does it avoid the documented antipatterns?
      4. Code Quality - Is the code production-ready?

      ## Scenario
      #{scenario}

      ## Generated Code
      ```ruby
      #{generated_code}
      ```

      Respond with JSON only:
      {
        "overall_score": 4.5,
        "pass": true,
        "scores": {
          "correct_pattern": 5,
          "rails_conventions": 4,
          "avoids_antipatterns": 5,
          "code_quality": 4
        },
        "issues": ["list of specific issues found"],
        "suggestions": ["list of improvement suggestions"]
      }
    PROMPT
  end

  def judge_with_llm(provider:, prompt:)
    client = LLMClient.new(provider: provider)
    client.evaluate(prompt)
  end

  def cross_validate(scenario, generated_code, skill_name)
    # Use multiple LLMs to judge the same output
    openai_result = judge_with_llm(
      provider: :mock, # Change to :openai when implemented
      prompt: create_judge_prompt(skill_name, scenario, generated_code)
    )

    anthropic_result = judge_with_llm(
      provider: :mock, # Change to :anthropic when implemented
      prompt: create_judge_prompt(skill_name, scenario, generated_code)
    )

    {
      openai: openai_result,
      anthropic: anthropic_result,
      average_score: (
        openai_result["overall_score"] + anthropic_result["overall_score"]
      ) / 2.0,
      agreement: openai_result["pass"] == anthropic_result["pass"]
    }
  end
end

# Include helpers in test classes
Minitest::Test.include SkillTestHelpers
Minitest::Test.include LLMJudgeHelpers
